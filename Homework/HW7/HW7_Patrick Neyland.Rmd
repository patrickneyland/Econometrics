---
title: "HW7 Patrick Neyland"
author: "Patrick Neyland"
date: '2022-10-18'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
library(tidyverse)
library(wooldridge)
library(stargazer)
library(car)
```

## Question 1
(i) and (iii) can cause the usual OLS t statistic to be invalid. 

## Question 2
### Part i   

$H_0: \beta_3 = 0$ $H_1: \beta_3 = 0$

### Part ii   
I expect both $\beta_1$ and $\beta_2$ to be positive. 

### Part iii   
10% increase in pop will result in .66% increase in rent, not 6.6% as stated in the problem. Because it is a log-log equation, there is no scaling of the coefficients. 

### Part iv
```{r}
.0056/.0017
qt(.99, 60)
```
$$t = \frac{0.0056-0}{0.0017} = 3.294$$
$$c = 2.39$$
Because $t > c$ we reject the null hypothesis at the 1% significance level. 

Or we can compute the p-value. 

```{r}
pt(3.294, 60, lower.tail = FALSE)
```
Easily reject the null because the p-value(0.00083) is less than 0.01($\alpha$)


## Question 3
### Part i

$Var(\hat\beta_1 - 3\hat\beta_2) = Var(\hat\beta_1) + 9Var(\hat\beta_2) - 6Cov(\hat\beta_1,\hat\beta_2)$

$se(\hat\beta_1 - 3\hat\beta_2) = [{[se(\hat\beta_1)]^2 + [se(\hat\beta_2)]^2 - 2s_{12}}]^{1/2}$


### Part ii

$t = \frac{(\hat\beta_1-3\hat\beta_2)-1}{se(\hat\beta_1-3\hat\beta_2)-1}$

### Part iii

$\hat y = (\beta_1 - 3\beta_2) + \beta_3$


## Question 4 
### Part i
### Part ii
q = 2   
$F = \frac{209,444.99 - 165,644.51/2}{165,644.51/86}$
```{r}
((209444.99-165644.51)/2)/(165644.51/86)
1 - pf(11.37026, 2, 86)
```

### Part iii
$F = \frac{.829 - .820 / 3}{1-.829/83}$
```{r}
((.829-.820)/3)/(1-(.829/83))
```

### Part iv
Homoscedasticity assumption is violated. You cannot count on the f-statistic to fit the F distribution---you cannot rely on the F test.

## Question 5
### Part i
```{r}
df <- filter(k401ksubs, fsize == 1)
length(df$fsize)
```
The $k401ksubs$ data set contains 2017 single person households. 


### Part ii
```{r}
model5_2 <- lm(nettfa ~ inc + age, data = df)
stargazer(model5_2, type = "text")
```
No, there are no surprises in the the slope estimates. 

### Part iii

I think it shows that a lot of people start out with a lot of debt early in their careers---student loans, mortgage, etc. 

### Part iv

```{r}
linearHypothesis(model5_2, "age=1")
```
The p-value is 0.08743. At the 1% signigicance level, we do not reject the null hypothesis $H_0: \beta_2 = 1$
### Part v
```{r}
model5_5 <- lm(nettfa ~ inc, data = df)
stargazer(model5_5, type = "text")
```
The estimated coefficient on $inc$ is not much different from the estimate in part ii. 

## Question 6 
### Part i
```{r}
model6_1 <- lm(log(psoda) ~ prpblck + log(income) + prppov, data = discrim)
stargazer(model6_1, type = "text")
```
Yes, $\hat{\beta_1}$ is statistically different from zero at a 5% level in a two-sided test. However, it is not significant at the 1% level.

### Part ii
```{r}
df1 <- na.omit(discrim)
cor(df1$lincome, df1$prppov)
cor(log(df1$income), df1$prppov)
summary(model6_1)
```
The correlation between $log(income)$ and $prppov$ is -0.847
Yes, each of the variables is statistically significant.

| Variable    | p-value |   |   |   |
|-------------|---------|---|---|---|
| prpblck     | 0.018   |   |   |   |
| log(income) | 0.000   |   |   |   |
| prppov      | 0.004   |   |   |   |

```{r}
#df <- mutate()
model6_3 <- lm(log(psoda) ~ prpblck + I(prpblck+prppov) + log(income), data = discrim)
stargazer(model6_1, model6_3, type = "text")
```

```{r}
linearHypothesis(model6_1, c("prpblck = 0", "prppov = 0"))
```
### Part iv
```{r}
model6_4 <- lm(log(psoda) ~ prpblck + log(income) + 
                 prppov + log(hseval), data = discrim)
stargazer(model6_4, type = "text")
summary(model6_4)
```
Ceteris paribus, as the $hseval$ increases by 1%, $psoda$ increases by 0.121% on average.  The two-sided p-value for $H_0: \beta_{log(hseval)}=0$ is 2.67e-11.

### Part v
```{r}
linearHypothesis(model6_4, c("prpblck-prppov=0"))
```


