---
title: "HW8 Patrick Neyland"
author: "Patrick Neyland"
date: '2022-10-27'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(wooldridge)
library(stargazer)
library(car)
```

## Question 1
Positive bias if you drop risk tolerance

## Question 2
### Part i
```{r}
model1_1 <- lm(wage ~ educ + exper + tenure, data = wage1)
res_model1_1 <- resid(model1_1)
hist(res_model1_1, breaks = 20)
```

### Part ii
```{r}
model1_2 <- lm(log(wage) ~ educ + exper + tenure, data = wage1)
res_model1_2 <- resid(model1_2)
hist(res_model1_2, breaks = 20, c = "blue")
```

### Part iii
I believe that MLR.6 is closer to being satisfied in the log-level model


## Question 3
### Part i
Logically, the range of scores would be 0 to 100.
```{r}
range(econmath$score)
```
In the sample, the scores range form 19.53 to 98.44. 

### Part ii
```{r}
reg3_1 <- lm(score ~ colgpa +actmth + acteng, econmath)
hist(reg3_1$residuals)
```
Because the histogram for the residuals is not normally distributed. This means that the t-statistic is not going to have a t distribution. 


### Part iii

```{r}
summary(reg3_1)
```
The t-stat is `r 0.052/0.111`. The p-value is 0.641. 
We don't need MLR.6 if the sample size is large enough. The central limit theorem kicks in and the distribution of the error term does not matter. 